**Note:** This repository is associated with MISR UBC Data Analytics & AI Research Group

## Preliminary


Skills associated with this git repository:
- Transformer models including **Bidirectional Encoder Representation from Transformers (BERT), BigBird, Longformer**
- 


## Terminology
**Incentivized Review**: 

Bidirectional Encoder Representations from Transformers (BERT) was used to predict whether a review is incentivized or not incentivized using imported dataset form Qiao et al. (2020).

## Pre-processing Data
- reviews (including )
- sample reviews were gathered as .csv file from ~~~, and disclosure sentences were masked.
<<<<<<< HEAD


## BERT Specification
- hidden_size: 1024,
- intermediate_size: 4096
- num_attention_heads: 16
- num_hidden_layers: 24
- hidden_act: "gelu"

## Process of the Workflow
- Raw data (in .csv file format) 

## Range of learning rate testing

Initially, hyperparameters were set to:
-

